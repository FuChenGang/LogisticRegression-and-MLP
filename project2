import numpy as np
import math
class MLP:
    w = None
    target_type = None
    learning_rate = 0

    hidden_layer_number = None
    the_number_of_units_in_each_layer = None
    
    def sigmod(self,x):
        return  1/(1+np.exp(-x))
    def sigmod_derivatiwve(self,x):
        return np.exp(x)/np.square(1+np.exp(-x))
    #sigmod = lambda x:  1/(1+np.exp(-x))#sigmod激活函数
    # sigmod_derivatiwve = lambda x:np.exp(x)/np.square(1+np.exp(-x))#sigmod导数
    


    def __init__(self,hidden_layer_number,the_number_of_units_in_each_layer,feature_number,learning_rate):
        self.hidden_layer_number = hidden_layer_number
        self.the_number_of_units_in_each_layer = the_number_of_units_in_each_layer
        self.learning_rate = learning_rate
        #k = len(x[0])#k为x的feature数
        self.w = []
        a = 0
        while a < hidden_layer_number:
            units_number = the_number_of_units_in_each_layer[a]
            
            if a == 0:
                w_matrix = np.zeros([feature_number+1,units_number])#初始化学习参数为0
                self.w.append(w_matrix)
            else:
                units_number2 = the_number_of_units_in_each_layer[a-1]
                w_matrix = np.zeros([units_number2 + 1,units_number])#初始化学习参数为0
                self.w.append(w_matrix)
            a = a + 1
        #初始化学习参数为0:输出层
        if hidden_layer_number == 0:
            w_matrix = np.zeros(feature_number + 1)
            self.w.append(w_matrix)
        else:
            w_matrix = np.zeros(the_number_of_units_in_each_layer[-1] + 1)
            self.w.append(w_matrix)
        print('w:',self.w)

        
        


    def data_proccessed_x(self,x):
    #将矩阵x拓展一列用来乘偏置项w0并返回处理后的矩阵
        a = x.shape
        x_proccessed = np.ones([a[0],a[1]+1])
        x_proccessed[:,1:] = x
        return x_proccessed

    def data_proccessed_target(self,target):
        target_type = set()
        for i in target:
            target_type.add(i)
        target_type = list(target_type)
        self.target_type = target_type#记录

    def hidden_layer(self,x,w_matrix):#激活函数为sigmod函数的隐藏层,w_matrix为(feature+1)*unit_number
        x_matrix = self.data_proccessed_x(x)#x为n个样本*(feature + 1)
        z_matrix = x_matrix.dot(w_matrix)#对x线性变换后（n个样本） * （unit_number)
        
        y_matrix = self.sigmod(z_matrix)
        y_derivative_of_z = self.sigmod_derivatiwve(z_matrix)

        print('z:',z_matrix)



        z_derivative_of_x_matrix = []
        z_ki_derivative_of_x_matrix = x_matrix.copy()
        aa = z_matrix.shape[0]
        bb = z_matrix.shape[1]
        cc = x_matrix.shape[0]
        dd = x_matrix.shape[1]
        k = 0
        while k < aa:
            i = 0
            z1 = []
            while i < bb :
                m = 0
                while m < cc:
                    n = 0
                    while n < dd:
                        if m != k :
                            z_ki_derivative_of_x_matrix[m][n] = 0
                        else:
                            z_ki_derivative_of_x_matrix[m][n] = w_matrix[n][i]
                        n = n + 1
                    m = m + 1
                y_derivative_of_x_matrix =  z_ki_derivative_of_x_matrix * y_derivative_of_z[k][i]
                z1.append(y_derivative_of_x_matrix)
                i = i + 1
            z_derivative_of_x_matrix.append(z1)
            k = k + 1
        y_derivative_of_x_matrix = np.array(z_derivative_of_x_matrix)#数列转回数组


        z_derivative_of_w_matrix = []
        z_ki_derivative_of_w_matrix = w_matrix.copy()
        aa = z_matrix.shape[0]
        bb = z_matrix.shape[1]
        cc = w_matrix.shape[0]
        dd = w_matrix.shape[1]
        k = 0
        while k < aa:
            i = 0
            z1 = []
            while i < bb :
                m = 0
                while m < cc:
                    n = 0
                    while n < dd:
                        if n != i :
                            z_ki_derivative_of_w_matrix[m][n] = 0
                        else:
                            # print(f'm:{m},n:{n},k:{k}')
                            # print(f'x_matrix:{x_matrix}\nw_matrix{w_matrix}\nz_matrix{z_matrix}')
                            z_ki_derivative_of_w_matrix[m][n] = x_matrix[k][m]
                        n = n + 1
                    m = m + 1
                y_derivative_of_w_matrix =  z_ki_derivative_of_w_matrix * y_derivative_of_z[k][i]
                z1.append(y_derivative_of_w_matrix)
                i = i + 1
            z_derivative_of_w_matrix.append(z1)
            k = k + 1
        y_derivative_of_w_matrix = np.array(y_derivative_of_w_matrix)#数列转回数组
        return([y_matrix,y_derivative_of_w_matrix,y_derivative_of_x_matrix])
        

        

    
    

    def train_output_layer(self,x,t):#输出层
        self.data_proccessed_target(t)
        loss = math.inf
        terminate = 0
        x = self.data_proccessed_x(x)
        derivative = np.zeros([len(self.w[-1])])
        while loss > 0.05:#训练继续进行的条件
            
            
            z_matrix = self.w[-1].dot(x.T)
            z_matrix = z_matrix.T#转置 
            y = 1/(1+np.exp(-z_matrix))
            
            print('\nz_matrix:',z_matrix)
            print('np.exp(-z_matrix):',np.exp(-z_matrix))
            
            loss = self.loss(y,t)

            print(f"\nloss{terminate}:",loss)
            k = 0
            print('np.sum((-y + t)* x[:0]):',np.sum((-y + t)* x[:,0]))
            while k < len(self.w[-1]):
                    #derivative_matrix[k][i] = np.sum((y[:,k]-self.target_proccessed[:,k])* x_matrix[:,i]) +  self.w[-1][k][i]#正则项
                regulazation = 10000*self.w[-1][k]
                derivative[k] = np.sum(((y-1) + t)* x[:,k])   + regulazation#正则化
                k = k + 1
            #参数更新：
            self.w[-1] = self.w[-1] - self.learning_rate * derivative
            terminate = terminate + 1
            print("迭代次数:",terminate)
            if terminate >= 200:#最大训练次数
                return 
    
    def loss(self,y,t):
        
        loss = -np.sum(t * np.log(1-y))/len(y)-np.sum((1-t) * np.log(y))/len(y)
        
        return loss
    


    def predict(self,x):
        x_matrix = self.data_proccessed_x(x)
        z_matrix = self.w[-1].dot(x_matrix.T)
        y = 1/(1+np.exp(-z_matrix))#指数处理
        output = []
        for i in y:
            if i >= 0.5:
                output.append(self.target_type[0])
            else:
                output.append(self.target_type[1])
        return output
            

        



filepath = "C:\\Users\\86136\\Desktop\\杂物\\人工智能与机器学习\\作业\\作业8\\新建 文本文档.txt"
data = np.loadtxt(filepath,dtype = int,delimiter=',')
x = data[:,0:-1]
target = data[:,-1]
a = 0
for i in target:
    if i < 5 :
        target[a] = 0
    else:
        target[a] = 1
    a = a + 1
MLP = MLP(1,[2],2,0.0000002)

#MLP.train_output_layer(x,target)
MLP.hidden_layer(x,MLP.w[0])


#y = MLP.predict(x)
#print("训练集的预测值：\n",y)
#a = y - target
wrong_number = np.sum(np.absolute(a))
print("训练集的错误个数:",wrong_number)
print("对于训练集的预测准确率：",(len(target)-wrong_number) / len(target))

# filepath2 = "C:\\Users\\86136\\Desktop\\杂物\\人工智能与机器学习\\作业\\作业8\\optdigits.tes"
# data = np.loadtxt(filepath2,dtype = int,delimiter=',')
# x = data[:,0:-1]
# target = data[:,-1]
# a = 0
# for i in target:
#     if i < 5 :
#         target[a] = 0
#     else:
#         target[a] = 1
#     a = a + 1
# y = MLP.predict(x)
# print("测试集的预测值：\n",y)
# a = y - target
# wrong_number = np.sum(np.absolute(a))
# print("测试集的错误个数:",wrong_number)
# print("对于测试集的预测准确率：",(len(target)-wrong_number) / len(target))